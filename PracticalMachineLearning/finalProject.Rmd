---
title: "Practical Machine Learning final project"
output: html_notebook
---

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Model description

In this scenario our outcome vector is a 5-factor one-hot representation describing a particular method used for "one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl". The five possibilities of the factor are:

Class A: exactly according to the specification.
Class B: throwing the elbows to the front.
Class C: lifting the dumbbell only halfway.
Class D: lowering the dumbbell only halfway.
Class E: throwing the hips to the front.

The goal of our model is to predict, from other variables, which of the 5 possible methods were used during a particular sample. Evaluating predictions will be based on maximizing the accuracy within the training set while minimizing the out-of-sample testing set error.

Using all remaining variables after cleaning the data set we will create both a simple decision tree model and a random forest model. In the end we will compare these two models and prefer the one that gives the greatest training set accuracy and minimizes the out-of-sample testing set error.

### Cross Validation

We will employ cross-validation during our model testing in order to minimize our out-of-sample error. This will be performed by subsampling the training set into a training and validation set at 60% and 40% proportions, respectively. This technique will allow us to train both competing models on the same data while withholding the testing set for final evaluations.

### Expected error

The error we expect in the out-of-sample predictions is equal to 1 minus the accuracy of the model we choose to predict with, 1-accuracy, in the cross-validation set. We will define accuracy as the proportion of correctly predicted samples over the total number of samples. In other words we expect the out-of-sample error rate to be approximately the classification error experienced in the cross-validation set.

## Setting up environment and cleaning the data

```{r echo=FALSE}
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(randomForest)
library(RColorBrewer)
set.seed(03052017)
```

```{r}
trainDatasetURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testDatasetURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainingDF <- read.csv(url(trainDatasetURL))
trainingDFOutcome <- trainingDF$classe
testingDF <- read.csv(url(testDatasetURL))
testingDFOutcome <- testingDF$classe
```

In its raw format the training set has dimension

```{r}
dim(trainingDF)
```

The testing set has dimension

```{r}
dim(testingDF)
```

```{r, echo=FALSE}
rem <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 
  'num_window', 'kurtosis_yaw_belt', 'skewness_yaw_forearm', 'skewness_pitch_forearm', 
  'skewness_roll_forearm', 'kurtosis_yaw_forearm', 'kurtosis_picth_forearm', 'kurtosis_roll_forearm', 
  'amplitude_yaw_dumbbell', 'min_yaw_dumbbell', 'max_yaw_dumbbell', 'skewness_yaw_dumbbell', 
  'skewness_pitch_dumbbell', 'skewness_roll_dumbbell', 'skewness_roll_dumbbell', 'kurtosis_yaw_dumbbell', 
  'kurtosis_picth_dumbbell', 'kurtosis_roll_dumbbell', 'skewness_yaw_arm', 'skewness_pitch_arm', 
  'skewness_roll_arm', 'kurtosis_yaw_arm', 'kurtosis_picth_arm', 'kurtosis_roll_arm', 'kurtosis_roll_belt',
  'kurtosis_picth_belt', 'skewness_roll_belt', 'skewness_roll_belt.1', 'skewness_yaw_belt', 
  'amplitude_yaw_belt', 'min_yaw_forearm', 'amplitude_yaw_forearm', 'min_yaw_belt', 'max_yaw_belt',
  'max_yaw_forearm')
trainingDF <- trainingDF[, !(names(trainingDF) %in% rem)]
testingDF <- testingDF[, !(names(testingDF) %in% rem)]
```

There are also many columns with very many missing values. This code will remove columns with > 70% NAs.

```{r}
trainingNA <- is.na(trainingDF)
indexesToRemove <- c()
for(i in 1:dim(trainingNA)[2]) {
  tooManyNAs <- sum(trainingNA[, i]) / dim(trainingNA)[1] > 0.7
  if (tooManyNAs) {
    indexesToRemove <- c(indexesToRemove, i)
  }
}
trainingDF <- trainingDF[, -indexesToRemove]
testingDF <- testingDF[, -indexesToRemove]
```

The cleaned up training set's dimensions are

```{r}
dim(trainingDF)
```

The cleaned up testing set's dimensions are

```{r}
dim(testingDF)
```

### Partitioning training and validation set

```{r}
trainingDF$classe <- trainingDFOutcome
trainingSubsetIndexes <- createDataPartition(y=trainingDF$classe, p=0.6, list=FALSE)
trainingSubsetDF <- trainingDF[trainingSubsetIndexes, ]
trainingSubsetDF <- trainingSubsetDF[, !(names(trainingSubsetDF) %in% c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window'))]
validationDF <- trainingDF[-trainingSubsetIndexes, ]
validationDF <- validationDF[, !(names(validationDF) %in% c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window'))]
testingDF <- testingDF[, !(names(testingDF) %in% c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window'))]
print(dim(trainingSubsetDF)[1] / dim(trainingDF)[1])
print(dim(validationDF)[1] / dim(trainingDF)[1])
```

## Model 1: Decision tree

Now we fit our data to a decision tree model.

```{r, cache=TRUE}
outcome <- trainingSubsetDF$classe
trainingSubsetDFMin <- trainingSubsetDF[, 1:dim(trainingSubsetDF)[2]-1]
decisionTreeFit <- rpart(outcome ~ ., data=trainingSubsetDFMin, method="class")
```

Plotting the model fit.

```{r}
fancyRpartPlot(decisionTreeFit)
```

### Predictions

```{r}
predictionsModel1 <- predict(decisionTreeFit, validationDF, type='class')
confusionMatrix(predictionsModel1, validationDF$classe)
```

This model has decent accuracy at 73.22%. Since this model did not do as well as we would hope let's attempt to model a random forest, which takes a decision tree and repeats the process multiple times, averaging the results across all attempts.

## Model 2: Random forests

```{r}
randomForestFit <- randomForest(outcome ~ ., data=trainingSubsetDFMin)
```

### Predictions

```{r}
predictionsModel2 <- predict(randomForestFit, validationDF, type='class')
confusionMatrix(predictionsModel2, validationDF$classe)
```

Much better! It looks like we were able to achieve 99.26% accuracy against the validation set with this model.

## Conclusion

It looks like the random forest model has a much greater accuracy than a simple decision tree. Let's now perform the final model predictions using the withheld test set on Model 2 to get the answers to the quiz.

```{r}
predictionsModel2Final <- predict(randomForestFit, testingDF, type='class')
print(predictionsModel2Final)
```

```{r, echo=FALSE}
pml_write_files = function(x) {
  n = length(x)
  for (i in 1:n) {
    filename = paste0('problem_id_',i,'.txt')
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsModel2Final)
```
